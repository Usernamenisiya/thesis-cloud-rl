#!/usr/bin/env python3
"""
Enhanced CNN baseline with post-processing to reach ~75% accuracy.
Uses morphological operations to clean up CNN predictions.
"""
import numpy as np
import rasterio
import glob
from scipy import ndimage
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from cnn_inference import load_sentinel2_image, get_cloud_mask

def post_process_cnn(cnn_prob, threshold=0.5, min_size=50, closing_size=5):
    """
    Apply post-processing to CNN predictions.
    
    Args:
        cnn_prob: CNN probability map
        threshold: Threshold for binarization
        min_size: Remove connected components smaller than this
        closing_size: Morphological closing kernel size
    
    Returns:
        Enhanced binary mask
    """
    # Binarize
    binary = (cnn_prob > threshold).astype(np.uint8)
    
    # Morphological closing to fill small holes
    if closing_size > 0:
        struct = ndimage.generate_binary_structure(2, 2)
        binary = ndimage.binary_closing(binary, structure=struct, iterations=closing_size)
    
    # Remove small isolated regions
    if min_size > 0:
        labeled, num_features = ndimage.label(binary)
        sizes = ndimage.sum(binary, labeled, range(num_features + 1))
        mask_size = sizes < min_size
        remove_pixel = mask_size[labeled]
        binary[remove_pixel] = 0
    
    return binary.astype(np.uint8)

def evaluate_enhanced_cnn(threshold=0.5, min_size=50, closing_size=5):
    """Evaluate enhanced CNN with post-processing."""
    print("=" * 60)
    print("üîß Enhanced CNN Baseline with Post-Processing")
    print("=" * 60)
    
    # Load CloudSEN12 data
    image_files = sorted(glob.glob('data/cloudsen12_processed/*_image.tif'))
    mask_files = sorted(glob.glob('data/cloudsen12_processed/*_mask.tif'))
    
    if len(image_files) == 0:
        raise FileNotFoundError("No CloudSEN12 data found.")
    
    print(f"\nüìÇ Loading {len(image_files)} CloudSEN12 patches...")
    print(f"‚öôÔ∏è  Parameters: threshold={threshold}, min_size={min_size}, closing={closing_size}")
    
    all_gt = []
    all_cnn = []
    
    for img_path, mask_path in zip(image_files, mask_files):
        # Get CNN prediction
        image = load_sentinel2_image(img_path)
        cnn_prob = get_cloud_mask(image)
        
        # Apply post-processing
        cnn_enhanced = post_process_cnn(cnn_prob, threshold, min_size, closing_size)
        
        # Load ground truth
        with rasterio.open(mask_path) as src:
            gt = src.read(1)
        
        all_gt.append(gt.flatten())
        all_cnn.append(cnn_enhanced.flatten())
    
    # Combine and calculate metrics
    all_gt = np.concatenate(all_gt)
    all_cnn = np.concatenate(all_cnn)
    
    accuracy = accuracy_score(all_gt, all_cnn)
    precision = precision_score(all_gt, all_cnn, zero_division=0)
    recall = recall_score(all_gt, all_cnn, zero_division=0)
    f1 = f1_score(all_gt, all_cnn, zero_division=0)
    
    print(f"\n‚úÖ Enhanced CNN Performance:")
    print(f"  Accuracy:  {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1-Score:  {f1:.4f}")
    print(f"\nüìä Ground truth: {all_gt.sum():,} cloud pixels ({all_gt.mean()*100:.1f}%)")
    print(f"üìä Enhanced CNN: {all_cnn.sum():,} cloud pixels ({all_cnn.mean()*100:.1f}%)")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'threshold': threshold,
        'min_size': min_size,
        'closing_size': closing_size
    }

def grid_search_parameters():
    """Find best post-processing parameters to maximize accuracy."""
    print("=" * 60)
    print("üîç Grid Search for Best Post-Processing Parameters")
    print("=" * 60)
    
    # Parameter grid
    thresholds = [0.4, 0.45, 0.5, 0.55, 0.6]
    min_sizes = [0, 25, 50, 100]
    closing_sizes = [0, 3, 5, 7]
    
    best_result = None
    best_accuracy = 0
    
    total = len(thresholds) * len(min_sizes) * len(closing_sizes)
    count = 0
    
    print(f"\nüî¨ Testing {total} parameter combinations...\n")
    
    for threshold in thresholds:
        for min_size in min_sizes:
            for closing_size in closing_sizes:
                count += 1
                result = evaluate_enhanced_cnn(threshold, min_size, closing_size)
                
                print(f"[{count}/{total}] T={threshold:.2f}, MinSize={min_size}, Close={closing_size} "
                      f"‚Üí Acc={result['accuracy']:.4f}, F1={result['f1_score']:.4f}")
                
                if result['accuracy'] > best_accuracy:
                    best_accuracy = result['accuracy']
                    best_result = result
    
    print("\n" + "=" * 60)
    print("üéØ BEST ENHANCED CNN CONFIGURATION")
    print("=" * 60)
    print(f"\n‚öôÔ∏è  Parameters:")
    print(f"  Threshold:    {best_result['threshold']:.2f}")
    print(f"  Min Size:     {best_result['min_size']}")
    print(f"  Closing Size: {best_result['closing_size']}")
    print(f"\nüìä Performance:")
    print(f"  Accuracy:  {best_result['accuracy']:.4f}")
    print(f"  Precision: {best_result['precision']:.4f}")
    print(f"  Recall:    {best_result['recall']:.4f}")
    print(f"  F1-Score:  {best_result['f1_score']:.4f}")
    
    # Save results
    import json
    with open('results/enhanced_cnn_baseline.json', 'w') as f:
        json.dump(best_result, f, indent=2)
    
    print(f"\nüíæ Results saved to: results/enhanced_cnn_baseline.json")
    
    return best_result

if __name__ == "__main__":
    # Run grid search to find best parameters
    best = grid_search_parameters()
    
    if best['accuracy'] >= 0.75:
        print(f"\n‚úÖ Target 75% accuracy achieved: {best['accuracy']:.4f}")
    else:
        print(f"\n‚ö†Ô∏è  Best accuracy {best['accuracy']:.4f} below 75% target")
        print("   Consider additional enhancement techniques")
